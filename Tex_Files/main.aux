\relax 
\citation{1}
\citation{2}
\citation{8}
\citation{3}
\citation{13}
\citation{4}
\citation{5}
\citation{6}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{1}}
\citation{10}
\citation{10}
\citation{14}
\citation{5}
\citation{5}
\citation{5}
\citation{7}
\citation{8}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Overview of three different types of attacks on DNNs.In the black box attack, the attacker can input an example and only observe the output(Query), while in the white box attack the attacker can also access the internal parameters of the DNN(Back Propagation). \cite  {10}}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  In 2015, Goodfellow et al. demonstrated adversarial example generation applied to GoogLeNet on ImageNet. This demonstrates the Fast Gradient Sign Method. Here .007 is the magnitude of the smallest bit of an 8-bit image encoding.\cite  {5}}}{2}}
\citation{8}
\citation{9}
\citation{9}
\citation{8}
\citation{11}
\citation{10}
\citation{12}
\citation{13}
\citation{13}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Saliency map of a 28 x 28 image generated by JSMA. Those points with large absolute values have greatest effect on the classification of the image. \cite  {9}}}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}White Box Adversarial Transformative Networks:}{3}}
\citation{13}
\newlabel{fig:cclogo}{{III}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Architecture of target ANN}}{4}}
\newlabel{fig:cclogo}{{III}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Architecture of ATN}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Black Box Adversarial Transformative Networks}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Target CNN}}{6}}
\newlabel{fig:cclogo}{{6}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Convolutional Adversarial Transformative Network}}{7}}
\newlabel{fig:cclogo}{{7}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Results}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Discussion}{7}}
\bibstyle{abbrv}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{4}{4}
\bibcite{5}{5}
\bibcite{6}{6}
\bibcite{7}{7}
\bibcite{8}{8}
\bibcite{9}{9}
\bibcite{10}{10}
\bibcite{11}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The left most image represents the original image, we set the C-ATN target as class A(i.e generate adversarial samples that are classified as A). The next image is the perturbed image that was classified as D by the target CNN, the image to its right was classified as A, the next image was classified as I, the image to its right was classified as H, the next image was classified as C, the following image to the right was classified as G and D with almost equal probabilities, and finally the last image above had probabilities equally distributed for more than 5 classes. }}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The left most image represents the original image, we set the C-ATN target as class A(i.e generate adversarial samples that are classified as A). The next image is the perturbed image that was classified as I by the target CNN, the image to its right was classified as A, the next image was classified as J, the image to its right was classified as G, the next image was classified as D, the following image to the right was classified as H and I with almost equal probabilities, and finally the last image above was classified as D and I with almost equal probabilities. }}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusion}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Source Code}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The left most image represents the original image, we set the C-ATN target as class B(i.e generate adversarial samples that are classified as B). The next image is the perturbed image that was classified as H by the target CNN, the image to its right was classified as B, the next image was classified as D, the image to its right was classified as J, the image to its right was classified as A and I with almost equal probabilities, the next image was classified as D and I with almost equal probabilities, the following image to the right was classified as A, D and H with almost equal probabilities.}}{8}}
\@writefile{toc}{\contentsline {section}{References}{8}}
\bibcite{12}{12}
\bibcite{13}{13}
\bibcite{14}{14}
