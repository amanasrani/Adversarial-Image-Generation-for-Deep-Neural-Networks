\documentclass[12pt,a4paper]{article}
\usepackage[latin2]{inputenc}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{amsmath}
\begin{document}
\textbf{Adversarial Transformative Networks}

Adversarial Transformative Networks (ATNs) were proposed in 2017 by 
Baluja and Fischer in 2017 in their paper, Adversarial Transformative 
Networks: Learning to Generate Adversarial Examples. In contrast to 
other methods such as FGSM or JSMA that generate adversarial examples 
directly by computing gradients on the image pixels or solving an 
optimization problem on them, ATN is by itself, a neural network that 
transforms an input into an adversarial example against a target network 
or a set of networks. ATNs can be targeted or untargeted and can also be 
trained in a black-box or a white-box manner. 



We define the target neural network as y = f$_{w}$(x), with w 
parameter matrix of the network f, and x being the input. The output of 
f, y is a vector representing the class probabilities for each label. 
The ATN is formally defined as x' = g$_{f,}$$\theta $(x). $\theta 
$is the parameter matrix of the ATN and x' is the adversarial 
transformation of x, such that x' \~ x, but argmax f(x) ? f(x').



\textbf{Cost Function}

The parameters $\theta $ are obtained by solving an optimization on the 
cost function defined as follows:



Cost = argmin ? $\sum_{xi ?X}{}$$\beta $L$_{x}$(g$_{f,}$$
\theta $(x$_{i}$), x$_{i}$) + L$_{y}$(f(g$_{f,}$$
\theta $(x$_{i}$)),f(x))



$$L$_{x}$ is a measure of the difference between the generated 
adversarial example and the original input. L$_{y }$is a measure of 
the difference between the output generated by f on an adversarial 
example and a particular target output for a targeted attack. For an 
untargeted attack, L$_{y }$can be taken as a measure of likelihood 
between the output generated by f on an adversarial example to the 
output generated by f on the corresponding original input. In both 
cases, the goal is to identify values for $\theta $that minimize both 
L$_{x}$ and L$_{y}$, and thus, the entire cost function. $\beta 
$is tuning parameter to balance out the two terms.



For untargeted attacks, L$_{x}$ and L$_{y}$ can both be 
represented by straightforward L$_{1}$ or L$_{2}$ costs. For 
targeted attacks, L$_{x}$ can be a simple straightforward L$_{1}$ 
or L$_{2}$ loss, but L$_{y}$ would have to be modified to bias the 
learning towards the targeted task. To do this, the definition of L$_{
y}$ is modified to include a re-ranking function. Thus we could have a 
definition such as, 



L$_{y}$ = L$_{2}$(f(g$_{f,}$$\theta $(x$_{i}$)), 
r(f(x), t)



Here, r is the re-ranking function, and t is the target class that we're 
trying to make the classifier output. The choice of r() can be used to 
tune the rate of training. For instance, r can return a vector 
containing just a one-hot encoding, such that only the value 
corresponding to the target class is 1, with all others being 0. This 
approach however, does not make use of the probabilities that are 
computed by f to drive the biasing. Hence, a better re-ranking function 
would be,

r(f(x),t) = norm($\alpha $max(f(x); k = t, f(x); otherwise)



Where, norm is a standard normalization function, k represents the 
possible output classes and max() returns the max value of its input 
vector. $\alpha $ $>$ 1, is an additional parameter that ensures that 
the reranked output points only towards one particular class.



Once the parameters are obtained, the ATN can be used to generate a 
perturbation to each node of the input, thereby giving a new adversarial 
input. One point to note in the existing work on adversarial 
transformative networks is that, it does not restrict the input 
perturbation to lie stay within some defined neighbourhood, $
\varepsilon $, as used in FGSM. While the inputs are made to look as 
similar as possible by using the L$_{x}$ term in the cost function, 
there is no explicit bound on these perturbations, in terms of a 
neighbourhood. Thus in our experiments, we restrict the output nodes of 
the ATN to lie in the range $[$-1, 1$]$. These output nodes, multiplied 
by $\varepsilon $and added to the inputs would restrict the 
perturbations to be within a neighbourhood $\varepsilon $. This is 
mathematically formulated as:



x' = x + $\varepsilon $*g$_{f,}$$\theta $(x)



Since g$_{f,}$$\theta $(x) is quantifying the extent of 
perturbation as opposed to the actual value of the input after the 
perturbation, we modify the definition of the cost to have L$_{x}$ 
only include g$_{f,}$$\theta $(x), and L$_{y}$ to include f(x+g
$_{f,}$$\theta $(x)) as opposed to just f(g$_{f,}$$\theta $\\
(x)). Thus, the cost function, taking all choices into account would be:
$\sum_{xi ?X}{}$$\beta $L$_{x}$(g$_{f,}$$\theta $(x$_{i
}$)) + L$_{y}$(f(x$_{i}$+g$_{f,}$$\theta $(x$_{i}$
)),f(x$_{i}$\\
))\textbf{Training}

Training the ATN only requires the data that is to be perturbed, and 
knowledge of the targeted network. Knowledge of the targeted network 
requires both, the architecture of the trained network as well as all 
the weights used by this model. The original labels used by to train the 
targeted network are not required to do this training. 



Training involves standard forward propagation and backward propagation 
on the ATN, by computing the derivatives of the cost function defined 
above with respect to the parameters, $\theta $ of the ATN. Although 
derivatives with respect to the parameters, w of the target network are 
not required, these parameters are used as part of the chain rule while 
computing the derivatives with respect to $\theta .$ Computing the 
derivatives of L$_{x}$ is similar to what is followed in the standard 
back propagation to compute derivative of g with respect to $\theta $. 
Derivative of L$_{y}$ requires differentiating f with respect to $
\theta .$ This is done by differentiating f with respect to g and g 
with respect to $\theta .$ Computing the derivative of f with respect 
to g is the same as what is done in FGSM, since g is the input to f in L
$_{y}$. 



Mathematically,

$\delta Ly$/$\delta \theta $= 2*(f(x+g$_{f,}$$\theta $(x)) - 
r(f(x), t))$\delta f$/$\delta \theta $



$\delta f/\delta \theta $ = $(\delta f/\delta g)(\delta g/\delta 
\theta )$\\


Parameters $\theta $\\
in each layer of the ATN are updated in every iteration like with 
standard gradient descent.\textbf{Experiments}

We generated perturbed inputs from the non-MNIST dataset. Each image in 
the non-MNIST dataset contains of 28x28 pixel images, giving a total of 
785 features in the input, including the bias. We first ran our 
experiments to break a simple feed forward neural network, having 3 
layers. As mentioned above, the input layer consists of 785 nodes. These 
are mapped onto the hidden layer that has 1000 nodes. The 1000 nodes of 
the hidden layer are then mapped to 10 nodes in the output layer, with 
each node representing the probability of the input corresponding to a 
particular class (classes are alphabets from 'A' to 'J'). Sigmoid 
activations were used in both the hidden as well as the output layers. 
Upon training with 20000 samples, this feed forward neural network had 
an error of 0.34 on the training set.



The ATN used to break the network described above was also a 3-layered 
feed forward neural network. It had 785 nodes in the input layer, 1024 
nodes in the hidden layer and 784 nodes in the output layer. The 784 
nodes in the output layer each correspond to a perturbation to each of 
the 784 nodes in the input layer, without the bias term. We did not have 
any activations in the hidden layer and used tanh activation in the 
output layer. As mentioned earlier, tanh activations are used in the 
output layer to restrict the perturbations to be within the range of a 
neighbourhood, $\varepsilon $. We trained this model with a few 
different combinations for the choice of the reranking function between 
what was described above and a one hot encoding, and also the choice of 
using L$_{1}$ and L$_{2}$ for both L$_{x}$ and L$_{y}$. We 
were easily able to break this simple feed forward neural network, and 
increased the error rate to 0.9 in each of these different combinations.

\end{document}
